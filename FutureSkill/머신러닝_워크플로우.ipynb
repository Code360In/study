{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝 워크플로우.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr8Hg-uDtI4_"
      },
      "source": [
        "# 1번: Machine Learning Workflow\r\n",
        "#\r\n",
        "독립적인 단계를 통해 여러 데이터 과학자가 과도 한 처리 시간이 소모 계산 리소스 없이 동시에 동일한 파이프라인에서 작업할 수 있습니다. 또한 별도의 단계를 통해 각 단계에 서로 다른 계산 형식/크기를 쉽게 사용할 수 있는 장점\r\n",
        "#\r\n",
        "업무를 step by step으로 진행할 수 있으며, 그 과정에서 효율적인 시스템을 만들 수 있다.\r\n",
        "\r\n",
        "예를들어 workflow를 무시하고, pipeline부터 만들다보면 데이터의 형식이나 크기에 맞지않는 비효율적인 시스템이 나올 수 도 있으며,\r\n",
        "\r\n",
        "전처리과정에 따라 변화할 수 있는 여지들이 있음에도 불구하고 무작정 프로그램만 만들다보면 훨씬 비효율적인 시스템으로 만들어질 수 있음.\r\n",
        "\r\n",
        "때문에 업무의 효율성 뿐만아니라 시스템의 성능 측면에서도 workflow 를 만들고 가져가는 것이 필요하다고 생각됨.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvEc9GyrtsvM"
      },
      "source": [
        "# 2번: Iris 생태계 연구\r\n",
        "# 잠시 생태계를 연구하는 학자가 되어봅시다. Iris(붓꽃) 종의 생태를 연구하기 위해 Iris 종과 관련된 데이터를 수집하려고 합니다.\r\n",
        "# Iris의 종의 분류와 관련된 정리된 데이터셋이 있는지 찾아보고, 없을 경우엔 어떻게 데이터를 얻을 수 있을지 작성해보세요.\r\n",
        "\r\n",
        "scikit-learn의 datasets 모듈에 포함되어 있습니다. load_iris 함수를 사용해서 데이터를 적재"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsHBcWj9uXkR"
      },
      "source": [
        "# 3번 : Iris dataset\r\n",
        "# Iris 종 분류를 위한 데이터를 이미 수집하여 만들어 놓은 사례가 있습니다.\r\n",
        "# 1936년 Ronald Fisher의 논문 \"The use of multiple measurements in taxonomic problems\" 에서 Iris 종 분류를 위한 데이터가 처음 소개가 되었습니다.\r\n",
        "# Iris dataset이 어떤 목적으로 수집되어 만들어졌는지 조사한 내용을 정리해보세요.\r\n",
        " 붓꽃의 꽃잎petal과 꽃받침sepal의 폭과 길이를 센티미터 단위로 측정하여 들꽃에 있는 붓꽃의 품종을 구분하기 위해서 \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQvfSb2jvIgR"
      },
      "source": [
        "# 4번: Source the Iris dataset\r\n",
        "# Iris dataset을 얻을 수 있는 경로에는 크게 3가지가 있습니다.\r\n",
        "sklearn안에있는데 datasets를 통해서 편하게 불러올 수 있다. 이에 비해 kaggle이나 UCI는 데이터 파일을 다운받아야한다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "tGbj8XpSvpzR",
        "outputId": "e3066830-6806-48df-ed24-0c2a065e27e8"
      },
      "source": [
        "# 5번 : sklearn.load_iris\r\n",
        "#코드 구현의 편의성을 위해 scikit-learn에서 Iris dataset을 불러와서 사용하겠습니다.\r\n",
        "#sklearn.datasets.load_iris() 를 통해 데이터를 DataFrame으로 가져와 출력하는 코드를 작성하세요.\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "import pandas as pd \r\n",
        "# iris dataset을 DataFrame으로 불러오는 코드\r\n",
        "iris = load_iris()\r\n",
        "df = iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\r\n",
        "df['target'] = iris.target\r\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
              "0                  5.1               3.5  ...               0.2       0\n",
              "1                  4.9               3.0  ...               0.2       0\n",
              "2                  4.7               3.2  ...               0.2       0\n",
              "3                  4.6               3.1  ...               0.2       0\n",
              "4                  5.0               3.6  ...               0.2       0\n",
              "..                 ...               ...  ...               ...     ...\n",
              "145                6.7               3.0  ...               2.3       2\n",
              "146                6.3               2.5  ...               1.9       2\n",
              "147                6.5               3.0  ...               2.0       2\n",
              "148                6.2               3.4  ...               2.3       2\n",
              "149                5.9               3.0  ...               1.8       2\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "D3Q31D4Lvuvc",
        "outputId": "b8e9bdc5-8d27-4b4f-9538-e9cff3229bfe"
      },
      "source": [
        "iris = pd.DataFrame(load_iris().data, columns=load_iris().feature_names)\r\n",
        "iris"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                  5.1               3.5                1.4               0.2\n",
              "1                  4.9               3.0                1.4               0.2\n",
              "2                  4.7               3.2                1.3               0.2\n",
              "3                  4.6               3.1                1.5               0.2\n",
              "4                  5.0               3.6                1.4               0.2\n",
              "..                 ...               ...                ...               ...\n",
              "145                6.7               3.0                5.2               2.3\n",
              "146                6.3               2.5                5.0               1.9\n",
              "147                6.5               3.0                5.2               2.0\n",
              "148                6.2               3.4                5.4               2.3\n",
              "149                5.9               3.0                5.1               1.8\n",
              "\n",
              "[150 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2yx0enOwI4v",
        "outputId": "9243a2e5-ee51-4553-98ab-9056c2a7a00d"
      },
      "source": [
        "# 6번 : Estimate the model\r\n",
        "# 데이터를 얻었으니, 우선 모델을 통해 평가를 해봅니다.\r\n",
        "# Iris 종을 파악하기 위한 알맞는 모델을 선정한 뒤, 그 이유를 짧게 주석으로 작성해보세요.\r\n",
        "# 선택한 estimator를 sklearn을 사용하여 코드로 작성한 뒤, 간단한 성능 평가도 함께해보세요.\r\n",
        "# 선정한 estimator 불러오기\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Iris dataset 불러오기\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        " # training / test 데이터 세팅\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "# 불러온 estimator 객체 생성\r\n",
        "clf = LogisticRegression()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# estimator train, test and evaluate.\r\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqlPjDnJwZ46"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "# Iris dataset 불러오기\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        " # training / test 데이터 세팅\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "# 불러온 estimator 객체 생성\r\n",
        "clf = RandomForestClassifier()\r\n",
        "# estimator train, test and evaluate.\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "y_pred = clf.predict(X_test)\r\n",
        "acc = accuracy_score(y_pred, y_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm3JeMwNwhwU"
      },
      "source": [
        "# 7번 : check estimator\r\n",
        "#선택한 estimator를 통해 Iris dataset을 분류했을 때의 성능은 충분한가요?\r\n",
        "#만약 분류 모델의 달성 목표가 정확도 기준으로 0.95였다면, 목표를 달성하였나요?\r\n",
        "#그렇지 못했다면, 정확도를 올리기 위해 개선해야 할 포인트를 찾아서 작성해보세요.\r\n",
        "\r\n",
        "LogisticRegression을 기준으로 정확도가 96%로 기준을 만족하였다. 추가적인 파라미터 튜닝을 통해서 정확도를 올릴 수 있을 것이다\r\n",
        "\r\n",
        "Ridge나 Lasso Regression을 통하여 모델을 학습해본다. 각각의 alpha 값을 데이터셋에 맞게끔 grid search를 진행하여 본다.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Sc0BACw0W1"
      },
      "source": [
        "# 8번 : iris pre-processing 1\r\n",
        "#머신러닝에서 데이터는 feature vector로 표현이 되어 학습 및 평가에 사용됩니다.\r\n",
        "#feature vector로 표현이 될 때, 어떤 feature를 어떻게 사용할지에 대해서 정하는 과정을 feature engineering이라고 합니다.\r\n",
        "#대표적인 feature engineering 방법에 Normalization과 Standardization 있습니다.\r\n",
        "#아래 3문항에 대한 답변을 적어주세요.\r\n",
        "#① Iris dataset의 각 4개의 Feature의 의미와 특징에 대해서 서술하세요.\r\n",
        "#② Iris dataset을 normalization을 수행했을 때와 안했을 때의 차이에 대해 서술하세요. e.g. normalization 전후의 sepal length의 최대/최소값.\r\n",
        "#③ Iris dataset을 normalization 했을 때와 standardization을 했을 때의 차이에 대해 서술하세요.\r\n",
        "\r\n",
        "#꽃받침 길이 (cm),꽃받침 폭 (cm),꽃잎 길이 (cm),꽃잎 폭 (cm)\r\n",
        "#정규화가 표준화와 다른 가장 큰 특징은 모든 데이터가 0 ~ 1 사이의 값을을 가진다\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "iris = load_iris()\r\n",
        "x = iris['data']\r\n",
        "X = x[:, 0]\r\n",
        "X_ = (X - X.min()) / (X.max() - X.min())\r\n",
        "plt.figure(figsize=(12, 6))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "sns.distplot(X, bins=5)\r\n",
        "plt.title('Original', fontsize=16)\r\n",
        "\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "sns.distplot(X_, bins=5)\r\n",
        "plt.title('Normalization', fontsize=16)\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZZFXAvjyg3u"
      },
      "source": [
        "1. 각각의 꽃에 대하여 꽃받침의 길이와 넓이, 꽃잎의 길이와 넓이이다.\r\n",
        "\r\n",
        "Scaling은 각각의 Xi에 대하여 서로의 단위가 다를 때, 이를 맞춰주기 위하여 진행한다. 예를 들어, 키와 몸무게에 대한 데이터에서 키는 150이상의 데이터가 많은 반면, 몸무게는 100이하의 데이터가 많다. 이러한 경우, 단위를 보다 통일시켜주기위하여 scaling을 진행할 수 있다. 이를 통해 특정 변수에 의해 모델 결과가 크게 변화하는 현상을 방지할 수 있다.\r\n",
        "\r\n",
        "2. 이때 Normalization이란 한 변수의 최소값과 최대값을 0과 1로 두어 scaling해주는 것이다. max_value - x_value / max_value - min_value를 통해 수행한다.\r\n",
        "\r\n",
        "3. Standardization이란 한 변수의 데이터가 표준정규분포의 parameter인 평균 = 0, 분산 = 1이 되도록 변수를 scaling하는 것이다.\r\n",
        "\r\n",
        "이 때, 표준화와 정규화 중 어떤 scaling 방법이 더 좋냐에 대한 답은 없다. 데이터 셋에 맞게끔 둘 다 해보고 더 좋은 성능을 내는 방법을 택하는 것이 좋다.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I72JQ4YTyN3W",
        "outputId": "b5e62c33-2923-4053-925f-9baf855cc183"
      },
      "source": [
        "# 9번 : iris pre-processing 2\r\n",
        "# scikit-learn 라이브러리를 사용해 Normalization과 Standardization을 Iris dataset에 적용하는 코드를 작성하세요.\r\n",
        "\r\n",
        "# Min-max scaling, Standard scaling 라이브러리 불러오기\r\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\r\n",
        "\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "# X에 min-max scaling을 적용\r\n",
        "normalized = MinMaxScaler().fit_transform(X)\r\n",
        "\r\n",
        "# X에 standard scaling을 적용\r\n",
        "standardized = StandardScaler().fit_transform(X)\r\n",
        "\r\n",
        "print(normalized)\r\n",
        "print(standardized)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.22222222 0.625      0.06779661 0.04166667]\n",
            " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
            " [0.11111111 0.5        0.05084746 0.04166667]\n",
            " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
            " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
            " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
            " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
            " [0.02777778 0.375      0.06779661 0.04166667]\n",
            " [0.16666667 0.45833333 0.08474576 0.        ]\n",
            " [0.30555556 0.70833333 0.08474576 0.04166667]\n",
            " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
            " [0.13888889 0.41666667 0.06779661 0.        ]\n",
            " [0.         0.41666667 0.01694915 0.        ]\n",
            " [0.41666667 0.83333333 0.03389831 0.04166667]\n",
            " [0.38888889 1.         0.08474576 0.125     ]\n",
            " [0.30555556 0.79166667 0.05084746 0.125     ]\n",
            " [0.22222222 0.625      0.06779661 0.08333333]\n",
            " [0.38888889 0.75       0.11864407 0.08333333]\n",
            " [0.22222222 0.75       0.08474576 0.08333333]\n",
            " [0.30555556 0.58333333 0.11864407 0.04166667]\n",
            " [0.22222222 0.70833333 0.08474576 0.125     ]\n",
            " [0.08333333 0.66666667 0.         0.04166667]\n",
            " [0.22222222 0.54166667 0.11864407 0.16666667]\n",
            " [0.13888889 0.58333333 0.15254237 0.04166667]\n",
            " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
            " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
            " [0.25       0.625      0.08474576 0.04166667]\n",
            " [0.25       0.58333333 0.06779661 0.04166667]\n",
            " [0.11111111 0.5        0.10169492 0.04166667]\n",
            " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
            " [0.30555556 0.58333333 0.08474576 0.125     ]\n",
            " [0.25       0.875      0.08474576 0.        ]\n",
            " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
            " [0.16666667 0.45833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.5        0.03389831 0.04166667]\n",
            " [0.33333333 0.625      0.05084746 0.04166667]\n",
            " [0.16666667 0.66666667 0.06779661 0.        ]\n",
            " [0.02777778 0.41666667 0.05084746 0.04166667]\n",
            " [0.22222222 0.58333333 0.08474576 0.04166667]\n",
            " [0.19444444 0.625      0.05084746 0.08333333]\n",
            " [0.05555556 0.125      0.05084746 0.08333333]\n",
            " [0.02777778 0.5        0.05084746 0.04166667]\n",
            " [0.19444444 0.625      0.10169492 0.20833333]\n",
            " [0.22222222 0.75       0.15254237 0.125     ]\n",
            " [0.13888889 0.41666667 0.06779661 0.08333333]\n",
            " [0.22222222 0.75       0.10169492 0.04166667]\n",
            " [0.08333333 0.5        0.06779661 0.04166667]\n",
            " [0.27777778 0.70833333 0.08474576 0.04166667]\n",
            " [0.19444444 0.54166667 0.06779661 0.04166667]\n",
            " [0.75       0.5        0.62711864 0.54166667]\n",
            " [0.58333333 0.5        0.59322034 0.58333333]\n",
            " [0.72222222 0.45833333 0.66101695 0.58333333]\n",
            " [0.33333333 0.125      0.50847458 0.5       ]\n",
            " [0.61111111 0.33333333 0.61016949 0.58333333]\n",
            " [0.38888889 0.33333333 0.59322034 0.5       ]\n",
            " [0.55555556 0.54166667 0.62711864 0.625     ]\n",
            " [0.16666667 0.16666667 0.38983051 0.375     ]\n",
            " [0.63888889 0.375      0.61016949 0.5       ]\n",
            " [0.25       0.29166667 0.49152542 0.54166667]\n",
            " [0.19444444 0.         0.42372881 0.375     ]\n",
            " [0.44444444 0.41666667 0.54237288 0.58333333]\n",
            " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
            " [0.5        0.375      0.62711864 0.54166667]\n",
            " [0.36111111 0.375      0.44067797 0.5       ]\n",
            " [0.66666667 0.45833333 0.57627119 0.54166667]\n",
            " [0.36111111 0.41666667 0.59322034 0.58333333]\n",
            " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
            " [0.52777778 0.08333333 0.59322034 0.58333333]\n",
            " [0.36111111 0.20833333 0.49152542 0.41666667]\n",
            " [0.44444444 0.5        0.6440678  0.70833333]\n",
            " [0.5        0.33333333 0.50847458 0.5       ]\n",
            " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
            " [0.5        0.33333333 0.62711864 0.45833333]\n",
            " [0.58333333 0.375      0.55932203 0.5       ]\n",
            " [0.63888889 0.41666667 0.57627119 0.54166667]\n",
            " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
            " [0.66666667 0.41666667 0.6779661  0.66666667]\n",
            " [0.47222222 0.375      0.59322034 0.58333333]\n",
            " [0.38888889 0.25       0.42372881 0.375     ]\n",
            " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
            " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
            " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
            " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
            " [0.30555556 0.41666667 0.59322034 0.58333333]\n",
            " [0.47222222 0.58333333 0.59322034 0.625     ]\n",
            " [0.66666667 0.45833333 0.62711864 0.58333333]\n",
            " [0.55555556 0.125      0.57627119 0.5       ]\n",
            " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
            " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
            " [0.33333333 0.25       0.57627119 0.45833333]\n",
            " [0.5        0.41666667 0.61016949 0.54166667]\n",
            " [0.41666667 0.25       0.50847458 0.45833333]\n",
            " [0.19444444 0.125      0.38983051 0.375     ]\n",
            " [0.36111111 0.29166667 0.54237288 0.5       ]\n",
            " [0.38888889 0.41666667 0.54237288 0.45833333]\n",
            " [0.38888889 0.375      0.54237288 0.5       ]\n",
            " [0.52777778 0.375      0.55932203 0.5       ]\n",
            " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
            " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
            " [0.55555556 0.54166667 0.84745763 1.        ]\n",
            " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
            " [0.77777778 0.41666667 0.83050847 0.83333333]\n",
            " [0.55555556 0.375      0.77966102 0.70833333]\n",
            " [0.61111111 0.41666667 0.81355932 0.875     ]\n",
            " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
            " [0.16666667 0.20833333 0.59322034 0.66666667]\n",
            " [0.83333333 0.375      0.89830508 0.70833333]\n",
            " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
            " [0.80555556 0.66666667 0.86440678 1.        ]\n",
            " [0.61111111 0.5        0.69491525 0.79166667]\n",
            " [0.58333333 0.29166667 0.72881356 0.75      ]\n",
            " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
            " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
            " [0.41666667 0.33333333 0.69491525 0.95833333]\n",
            " [0.58333333 0.5        0.72881356 0.91666667]\n",
            " [0.61111111 0.41666667 0.76271186 0.70833333]\n",
            " [0.94444444 0.75       0.96610169 0.875     ]\n",
            " [0.94444444 0.25       1.         0.91666667]\n",
            " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
            " [0.72222222 0.5        0.79661017 0.91666667]\n",
            " [0.36111111 0.33333333 0.66101695 0.79166667]\n",
            " [0.94444444 0.33333333 0.96610169 0.79166667]\n",
            " [0.55555556 0.29166667 0.66101695 0.70833333]\n",
            " [0.66666667 0.54166667 0.79661017 0.83333333]\n",
            " [0.80555556 0.5        0.84745763 0.70833333]\n",
            " [0.52777778 0.33333333 0.6440678  0.70833333]\n",
            " [0.5        0.41666667 0.66101695 0.70833333]\n",
            " [0.58333333 0.33333333 0.77966102 0.83333333]\n",
            " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
            " [0.86111111 0.33333333 0.86440678 0.75      ]\n",
            " [1.         0.75       0.91525424 0.79166667]\n",
            " [0.58333333 0.33333333 0.77966102 0.875     ]\n",
            " [0.55555556 0.33333333 0.69491525 0.58333333]\n",
            " [0.5        0.25       0.77966102 0.54166667]\n",
            " [0.94444444 0.41666667 0.86440678 0.91666667]\n",
            " [0.55555556 0.58333333 0.77966102 0.95833333]\n",
            " [0.58333333 0.45833333 0.76271186 0.70833333]\n",
            " [0.47222222 0.41666667 0.6440678  0.70833333]\n",
            " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
            " [0.66666667 0.45833333 0.77966102 0.95833333]\n",
            " [0.72222222 0.45833333 0.69491525 0.91666667]\n",
            " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
            " [0.69444444 0.5        0.83050847 0.91666667]\n",
            " [0.66666667 0.54166667 0.79661017 1.        ]\n",
            " [0.66666667 0.41666667 0.71186441 0.91666667]\n",
            " [0.55555556 0.20833333 0.6779661  0.75      ]\n",
            " [0.61111111 0.41666667 0.71186441 0.79166667]\n",
            " [0.52777778 0.58333333 0.74576271 0.91666667]\n",
            " [0.44444444 0.41666667 0.69491525 0.70833333]]\n",
            "[[-9.00681170e-01  1.01900435e+00 -1.34022653e+00 -1.31544430e+00]\n",
            " [-1.14301691e+00 -1.31979479e-01 -1.34022653e+00 -1.31544430e+00]\n",
            " [-1.38535265e+00  3.28414053e-01 -1.39706395e+00 -1.31544430e+00]\n",
            " [-1.50652052e+00  9.82172869e-02 -1.28338910e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00  1.24920112e+00 -1.34022653e+00 -1.31544430e+00]\n",
            " [-5.37177559e-01  1.93979142e+00 -1.16971425e+00 -1.05217993e+00]\n",
            " [-1.50652052e+00  7.88807586e-01 -1.34022653e+00 -1.18381211e+00]\n",
            " [-1.02184904e+00  7.88807586e-01 -1.28338910e+00 -1.31544430e+00]\n",
            " [-1.74885626e+00 -3.62176246e-01 -1.34022653e+00 -1.31544430e+00]\n",
            " [-1.14301691e+00  9.82172869e-02 -1.28338910e+00 -1.44707648e+00]\n",
            " [-5.37177559e-01  1.47939788e+00 -1.28338910e+00 -1.31544430e+00]\n",
            " [-1.26418478e+00  7.88807586e-01 -1.22655167e+00 -1.31544430e+00]\n",
            " [-1.26418478e+00 -1.31979479e-01 -1.34022653e+00 -1.44707648e+00]\n",
            " [-1.87002413e+00 -1.31979479e-01 -1.51073881e+00 -1.44707648e+00]\n",
            " [-5.25060772e-02  2.16998818e+00 -1.45390138e+00 -1.31544430e+00]\n",
            " [-1.73673948e-01  3.09077525e+00 -1.28338910e+00 -1.05217993e+00]\n",
            " [-5.37177559e-01  1.93979142e+00 -1.39706395e+00 -1.05217993e+00]\n",
            " [-9.00681170e-01  1.01900435e+00 -1.34022653e+00 -1.18381211e+00]\n",
            " [-1.73673948e-01  1.70959465e+00 -1.16971425e+00 -1.18381211e+00]\n",
            " [-9.00681170e-01  1.70959465e+00 -1.28338910e+00 -1.18381211e+00]\n",
            " [-5.37177559e-01  7.88807586e-01 -1.16971425e+00 -1.31544430e+00]\n",
            " [-9.00681170e-01  1.47939788e+00 -1.28338910e+00 -1.05217993e+00]\n",
            " [-1.50652052e+00  1.24920112e+00 -1.56757623e+00 -1.31544430e+00]\n",
            " [-9.00681170e-01  5.58610819e-01 -1.16971425e+00 -9.20547742e-01]\n",
            " [-1.26418478e+00  7.88807586e-01 -1.05603939e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00 -1.31979479e-01 -1.22655167e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00  7.88807586e-01 -1.22655167e+00 -1.05217993e+00]\n",
            " [-7.79513300e-01  1.01900435e+00 -1.28338910e+00 -1.31544430e+00]\n",
            " [-7.79513300e-01  7.88807586e-01 -1.34022653e+00 -1.31544430e+00]\n",
            " [-1.38535265e+00  3.28414053e-01 -1.22655167e+00 -1.31544430e+00]\n",
            " [-1.26418478e+00  9.82172869e-02 -1.22655167e+00 -1.31544430e+00]\n",
            " [-5.37177559e-01  7.88807586e-01 -1.28338910e+00 -1.05217993e+00]\n",
            " [-7.79513300e-01  2.40018495e+00 -1.28338910e+00 -1.44707648e+00]\n",
            " [-4.16009689e-01  2.63038172e+00 -1.34022653e+00 -1.31544430e+00]\n",
            " [-1.14301691e+00  9.82172869e-02 -1.28338910e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00  3.28414053e-01 -1.45390138e+00 -1.31544430e+00]\n",
            " [-4.16009689e-01  1.01900435e+00 -1.39706395e+00 -1.31544430e+00]\n",
            " [-1.14301691e+00  1.24920112e+00 -1.34022653e+00 -1.44707648e+00]\n",
            " [-1.74885626e+00 -1.31979479e-01 -1.39706395e+00 -1.31544430e+00]\n",
            " [-9.00681170e-01  7.88807586e-01 -1.28338910e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00  1.01900435e+00 -1.39706395e+00 -1.18381211e+00]\n",
            " [-1.62768839e+00 -1.74335684e+00 -1.39706395e+00 -1.18381211e+00]\n",
            " [-1.74885626e+00  3.28414053e-01 -1.39706395e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00  1.01900435e+00 -1.22655167e+00 -7.88915558e-01]\n",
            " [-9.00681170e-01  1.70959465e+00 -1.05603939e+00 -1.05217993e+00]\n",
            " [-1.26418478e+00 -1.31979479e-01 -1.34022653e+00 -1.18381211e+00]\n",
            " [-9.00681170e-01  1.70959465e+00 -1.22655167e+00 -1.31544430e+00]\n",
            " [-1.50652052e+00  3.28414053e-01 -1.34022653e+00 -1.31544430e+00]\n",
            " [-6.58345429e-01  1.47939788e+00 -1.28338910e+00 -1.31544430e+00]\n",
            " [-1.02184904e+00  5.58610819e-01 -1.34022653e+00 -1.31544430e+00]\n",
            " [ 1.40150837e+00  3.28414053e-01  5.35408562e-01  2.64141916e-01]\n",
            " [ 6.74501145e-01  3.28414053e-01  4.21733708e-01  3.95774101e-01]\n",
            " [ 1.28034050e+00  9.82172869e-02  6.49083415e-01  3.95774101e-01]\n",
            " [-4.16009689e-01 -1.74335684e+00  1.37546573e-01  1.32509732e-01]\n",
            " [ 7.95669016e-01 -5.92373012e-01  4.78571135e-01  3.95774101e-01]\n",
            " [-1.73673948e-01 -5.92373012e-01  4.21733708e-01  1.32509732e-01]\n",
            " [ 5.53333275e-01  5.58610819e-01  5.35408562e-01  5.27406285e-01]\n",
            " [-1.14301691e+00 -1.51316008e+00 -2.60315415e-01 -2.62386821e-01]\n",
            " [ 9.16836886e-01 -3.62176246e-01  4.78571135e-01  1.32509732e-01]\n",
            " [-7.79513300e-01 -8.22569778e-01  8.07091462e-02  2.64141916e-01]\n",
            " [-1.02184904e+00 -2.43394714e+00 -1.46640561e-01 -2.62386821e-01]\n",
            " [ 6.86617933e-02 -1.31979479e-01  2.51221427e-01  3.95774101e-01]\n",
            " [ 1.89829664e-01 -1.97355361e+00  1.37546573e-01 -2.62386821e-01]\n",
            " [ 3.10997534e-01 -3.62176246e-01  5.35408562e-01  2.64141916e-01]\n",
            " [-2.94841818e-01 -3.62176246e-01 -8.98031345e-02  1.32509732e-01]\n",
            " [ 1.03800476e+00  9.82172869e-02  3.64896281e-01  2.64141916e-01]\n",
            " [-2.94841818e-01 -1.31979479e-01  4.21733708e-01  3.95774101e-01]\n",
            " [-5.25060772e-02 -8.22569778e-01  1.94384000e-01 -2.62386821e-01]\n",
            " [ 4.32165405e-01 -1.97355361e+00  4.21733708e-01  3.95774101e-01]\n",
            " [-2.94841818e-01 -1.28296331e+00  8.07091462e-02 -1.30754636e-01]\n",
            " [ 6.86617933e-02  3.28414053e-01  5.92245988e-01  7.90670654e-01]\n",
            " [ 3.10997534e-01 -5.92373012e-01  1.37546573e-01  1.32509732e-01]\n",
            " [ 5.53333275e-01 -1.28296331e+00  6.49083415e-01  3.95774101e-01]\n",
            " [ 3.10997534e-01 -5.92373012e-01  5.35408562e-01  8.77547895e-04]\n",
            " [ 6.74501145e-01 -3.62176246e-01  3.08058854e-01  1.32509732e-01]\n",
            " [ 9.16836886e-01 -1.31979479e-01  3.64896281e-01  2.64141916e-01]\n",
            " [ 1.15917263e+00 -5.92373012e-01  5.92245988e-01  2.64141916e-01]\n",
            " [ 1.03800476e+00 -1.31979479e-01  7.05920842e-01  6.59038469e-01]\n",
            " [ 1.89829664e-01 -3.62176246e-01  4.21733708e-01  3.95774101e-01]\n",
            " [-1.73673948e-01 -1.05276654e+00 -1.46640561e-01 -2.62386821e-01]\n",
            " [-4.16009689e-01 -1.51316008e+00  2.38717193e-02 -1.30754636e-01]\n",
            " [-4.16009689e-01 -1.51316008e+00 -3.29657076e-02 -2.62386821e-01]\n",
            " [-5.25060772e-02 -8.22569778e-01  8.07091462e-02  8.77547895e-04]\n",
            " [ 1.89829664e-01 -8.22569778e-01  7.62758269e-01  5.27406285e-01]\n",
            " [-5.37177559e-01 -1.31979479e-01  4.21733708e-01  3.95774101e-01]\n",
            " [ 1.89829664e-01  7.88807586e-01  4.21733708e-01  5.27406285e-01]\n",
            " [ 1.03800476e+00  9.82172869e-02  5.35408562e-01  3.95774101e-01]\n",
            " [ 5.53333275e-01 -1.74335684e+00  3.64896281e-01  1.32509732e-01]\n",
            " [-2.94841818e-01 -1.31979479e-01  1.94384000e-01  1.32509732e-01]\n",
            " [-4.16009689e-01 -1.28296331e+00  1.37546573e-01  1.32509732e-01]\n",
            " [-4.16009689e-01 -1.05276654e+00  3.64896281e-01  8.77547895e-04]\n",
            " [ 3.10997534e-01 -1.31979479e-01  4.78571135e-01  2.64141916e-01]\n",
            " [-5.25060772e-02 -1.05276654e+00  1.37546573e-01  8.77547895e-04]\n",
            " [-1.02184904e+00 -1.74335684e+00 -2.60315415e-01 -2.62386821e-01]\n",
            " [-2.94841818e-01 -8.22569778e-01  2.51221427e-01  1.32509732e-01]\n",
            " [-1.73673948e-01 -1.31979479e-01  2.51221427e-01  8.77547895e-04]\n",
            " [-1.73673948e-01 -3.62176246e-01  2.51221427e-01  1.32509732e-01]\n",
            " [ 4.32165405e-01 -3.62176246e-01  3.08058854e-01  1.32509732e-01]\n",
            " [-9.00681170e-01 -1.28296331e+00 -4.30827696e-01 -1.30754636e-01]\n",
            " [-1.73673948e-01 -5.92373012e-01  1.94384000e-01  1.32509732e-01]\n",
            " [ 5.53333275e-01  5.58610819e-01  1.27429511e+00  1.71209594e+00]\n",
            " [-5.25060772e-02 -8.22569778e-01  7.62758269e-01  9.22302838e-01]\n",
            " [ 1.52267624e+00 -1.31979479e-01  1.21745768e+00  1.18556721e+00]\n",
            " [ 5.53333275e-01 -3.62176246e-01  1.04694540e+00  7.90670654e-01]\n",
            " [ 7.95669016e-01 -1.31979479e-01  1.16062026e+00  1.31719939e+00]\n",
            " [ 2.12851559e+00 -1.31979479e-01  1.61531967e+00  1.18556721e+00]\n",
            " [-1.14301691e+00 -1.28296331e+00  4.21733708e-01  6.59038469e-01]\n",
            " [ 1.76501198e+00 -3.62176246e-01  1.44480739e+00  7.90670654e-01]\n",
            " [ 1.03800476e+00 -1.28296331e+00  1.16062026e+00  7.90670654e-01]\n",
            " [ 1.64384411e+00  1.24920112e+00  1.33113254e+00  1.71209594e+00]\n",
            " [ 7.95669016e-01  3.28414053e-01  7.62758269e-01  1.05393502e+00]\n",
            " [ 6.74501145e-01 -8.22569778e-01  8.76433123e-01  9.22302838e-01]\n",
            " [ 1.15917263e+00 -1.31979479e-01  9.90107977e-01  1.18556721e+00]\n",
            " [-1.73673948e-01 -1.28296331e+00  7.05920842e-01  1.05393502e+00]\n",
            " [-5.25060772e-02 -5.92373012e-01  7.62758269e-01  1.58046376e+00]\n",
            " [ 6.74501145e-01  3.28414053e-01  8.76433123e-01  1.44883158e+00]\n",
            " [ 7.95669016e-01 -1.31979479e-01  9.90107977e-01  7.90670654e-01]\n",
            " [ 2.24968346e+00  1.70959465e+00  1.67215710e+00  1.31719939e+00]\n",
            " [ 2.24968346e+00 -1.05276654e+00  1.78583195e+00  1.44883158e+00]\n",
            " [ 1.89829664e-01 -1.97355361e+00  7.05920842e-01  3.95774101e-01]\n",
            " [ 1.28034050e+00  3.28414053e-01  1.10378283e+00  1.44883158e+00]\n",
            " [-2.94841818e-01 -5.92373012e-01  6.49083415e-01  1.05393502e+00]\n",
            " [ 2.24968346e+00 -5.92373012e-01  1.67215710e+00  1.05393502e+00]\n",
            " [ 5.53333275e-01 -8.22569778e-01  6.49083415e-01  7.90670654e-01]\n",
            " [ 1.03800476e+00  5.58610819e-01  1.10378283e+00  1.18556721e+00]\n",
            " [ 1.64384411e+00  3.28414053e-01  1.27429511e+00  7.90670654e-01]\n",
            " [ 4.32165405e-01 -5.92373012e-01  5.92245988e-01  7.90670654e-01]\n",
            " [ 3.10997534e-01 -1.31979479e-01  6.49083415e-01  7.90670654e-01]\n",
            " [ 6.74501145e-01 -5.92373012e-01  1.04694540e+00  1.18556721e+00]\n",
            " [ 1.64384411e+00 -1.31979479e-01  1.16062026e+00  5.27406285e-01]\n",
            " [ 1.88617985e+00 -5.92373012e-01  1.33113254e+00  9.22302838e-01]\n",
            " [ 2.49201920e+00  1.70959465e+00  1.50164482e+00  1.05393502e+00]\n",
            " [ 6.74501145e-01 -5.92373012e-01  1.04694540e+00  1.31719939e+00]\n",
            " [ 5.53333275e-01 -5.92373012e-01  7.62758269e-01  3.95774101e-01]\n",
            " [ 3.10997534e-01 -1.05276654e+00  1.04694540e+00  2.64141916e-01]\n",
            " [ 2.24968346e+00 -1.31979479e-01  1.33113254e+00  1.44883158e+00]\n",
            " [ 5.53333275e-01  7.88807586e-01  1.04694540e+00  1.58046376e+00]\n",
            " [ 6.74501145e-01  9.82172869e-02  9.90107977e-01  7.90670654e-01]\n",
            " [ 1.89829664e-01 -1.31979479e-01  5.92245988e-01  7.90670654e-01]\n",
            " [ 1.28034050e+00  9.82172869e-02  9.33270550e-01  1.18556721e+00]\n",
            " [ 1.03800476e+00  9.82172869e-02  1.04694540e+00  1.58046376e+00]\n",
            " [ 1.28034050e+00  9.82172869e-02  7.62758269e-01  1.44883158e+00]\n",
            " [-5.25060772e-02 -8.22569778e-01  7.62758269e-01  9.22302838e-01]\n",
            " [ 1.15917263e+00  3.28414053e-01  1.21745768e+00  1.44883158e+00]\n",
            " [ 1.03800476e+00  5.58610819e-01  1.10378283e+00  1.71209594e+00]\n",
            " [ 1.03800476e+00 -1.31979479e-01  8.19595696e-01  1.44883158e+00]\n",
            " [ 5.53333275e-01 -1.28296331e+00  7.05920842e-01  9.22302838e-01]\n",
            " [ 7.95669016e-01 -1.31979479e-01  8.19595696e-01  1.05393502e+00]\n",
            " [ 4.32165405e-01  7.88807586e-01  9.33270550e-01  1.44883158e+00]\n",
            " [ 6.86617933e-02 -1.31979479e-01  7.62758269e-01  7.90670654e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYj16xwSy08v"
      },
      "source": [
        "# 10번 : data-leakage\r\n",
        "# preprocessing 단계에서 다음과 같은 전처리를 수행하였습니다.\r\n",
        "# normalized_X = MinmaxScaler.fit_transform(X)\r\n",
        "# X_train, X_test, y_train, y_test = train_test_split(normalized_X, y, test_size=0.2)\r\n",
        "# 위 코드로 생성된 X_train 데이터를 기준으로 학습을 시킨 뒤, X_test를 테스트하면 어떤 문제가 생기는지 서술해주세요.\r\n",
        "테스트 데이터의 특성이 이미 전처리단계에서 스케일링 되었기 때문에 교차검증을 할 수 없다.\r\n",
        "\r\n",
        "머신러닝 문제에서 train set과 test set을 분리할 때, 무조건 test set은 완전히 새롭게 경험할 데이터여야 한다. 그렇지 않고 training 과정에서 test set에 대한 정보가 들어갈 경우, 모델의 일반화 성능을 측정하기 위하여 진행하는 test 단계의 의미가 무색된다.\r\n",
        "\r\n",
        "따라서, 위에 경우에 scaling 하는 경우에 있어서도 X_train, X_test 전부의 데이터를 통해서 scaling을 한 뒤에 나눠주면 안된다. Scaling을 진행하기 이전에 train set과 test set을 나눠준 뒤, train set에 대해서 scaling을 진행해줘야 한다.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxzu9JzkzlzZ",
        "outputId": "b178d5b1-bcfc-4c37-900d-48acadfc9e9d"
      },
      "source": [
        "# 11번 : Iris building pipeline\r\n",
        "# data leakage 문제를 해결하고, preprocessor와 estimator를 하나로 결합해주는 pipeline 구조를 만드는 코드를 작성하세요.\r\n",
        "# sklearn에서 pipeline 불러오기\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "# code from 10번\r\n",
        "# 위에서 만든 preprocessor와 estimator을 합치기\r\n",
        "pipe = make_pipeline(MinMaxScaler(), LogisticRegression())\r\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1JF03rSz8g5",
        "outputId": "d6542a49-77de-40c4-d7b5-39ace2b3dc1a"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier', LogisticRegression())])\r\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-yo7If0GrK"
      },
      "source": [
        "# 12번 : Evaluation Metric - 1\r\n",
        "# Iris dataset을 분류하는 문제의 성능을 측정하고자 합니다.\r\n",
        "# 분류 문제를 평가하는 방법에는 accuracy, precision 등 여러가지 방법이 있습니다.\r\n",
        "# accuracy, precision을 포함하여 분류 문제에서 사용하는 평가 방법을 4가지 이상 찾고, 평가 방법들의 정의와 의미를 정리해보세요.\r\n",
        "Accuracy ( 정확도 ) - 전체 데이터 중 올바르게 예측한 비율 - TP + TN / (TP + FN + FP + TN)\r\n",
        "Recall ( 재현율 ) - 실제 true 중 예측 true의 비율 - 예측 true / 실제 true = TP / (TP + FN)\r\n",
        "precision ( 정밀도 ) - 예측 true 중 실제 true의 비율 - 실제 true / 예측 true = TP / (TP + FP)\r\n",
        "F1 Score - 재현율과 정밀도의 조화평균 - 2 *precision*recall / ( precision + recall )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIviQmdn01dI"
      },
      "source": [
        "Precision (정밀도) : 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율\r\n",
        "Recall (재현율) : 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율\r\n",
        "Accuracy (정확도) : True-True뿐만 아니라 False-False라고 예측한 경우도 고려하는 지표\r\n",
        "F1-score : Precision과 Recall의 조화평균"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jutBe5c2022j",
        "outputId": "00eda077-7a88-4dbb-88e3-45fc456831d5"
      },
      "source": [
        "# 13번 : Evaluation Metric - 2\r\n",
        "# Iris dataset을 분류하는 문제의 성능을 측정하고자 합니다. 분류 문제의 성능을 측정하는 방법 중에 대표적인 방법인 Accuracy, Precision, Recall, F1-score을 측정하고자 합니다.\r\n",
        "# 5A에서 사용했던 코드를 이용하여 각 측정 방법대로 계산한 결과를 출력하는 코드를 작성하세요.\r\n",
        "###########################\r\n",
        "# Code from 6번\r\n",
        "\r\n",
        "# 선정한 estimator 불러오기\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Iris dataset 불러오기\r\n",
        "X, y = X, y = load_iris(return_X_y=True)\r\n",
        " # training / test 데이터 세팅\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "# 불러온 estimator 객체 생성\r\n",
        "clf = LogisticRegression()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "pred = clf.predict(X_test)\r\n",
        "\r\n",
        "# estimator train, test and evaluate.\r\n",
        "############################\r\n",
        "# Accuracy, Precision, Recall, F1 score 불러오기\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
        "\r\n",
        "# Accuracy\r\n",
        "acc = accuracy_score(y_test, y_pred)\r\n",
        "# Precision\r\n",
        "pre = precision_score(y_test, y_pred, average=  'weighted')\r\n",
        "# Recall\r\n",
        "re = recall_score(y_test, y_pred,average=  'weighted')\r\n",
        "# F1 score\r\n",
        "f1 = f1_score(y_test, y_pred,average=  'weighted')\r\n",
        "\r\n",
        "print(\"Accuracy : %.3f\" % acc)\r\n",
        "print(\"Precision : %.3f\" % pre)\r\n",
        "print(\"Recall : %.3f\" % re)\r\n",
        "print(\"F1 score : %.3f\" % f1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.300\n",
            "Precision : 0.333\n",
            "Recall : 0.300\n",
            "F1 score : 0.310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY_Cxxj91e4k",
        "outputId": "c1715bde-c809-4f22-9316-655403ff91e6"
      },
      "source": [
        "# 14번 : Model Validation\r\n",
        "#학습 데이터셋과 테스트 데이터셋만 2가지로 존재하는 경우에는 동일한 데이터셋에 대해서 계속 학습을 진행하다 보면, 테스트 데이터의 성능을 올리는 방향으로 모델이 과적합 되기 쉽습니다.\r\n",
        "#모델의 과적합을 막기 위해서, 테스트 데이터로 평가하기 이전에 평가를 하는 것으로 학습의 성능을 향상시키고 싶습니다.\r\n",
        "#학습 데이터의 일부를 validation data로 나누어 학습의 성능을 평가하는 용도로 사용하고자 합니다.\r\n",
        "#기존에 작성했던 코드를 활용하여 train data를 나누어 validation data를 만드는 코드를 작성하세요.\r\n",
        "\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "# train_test_split을 이용하여 X_train, X_val, X_test, y_train, y_val, y_test를 만드는 코드\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "X_val, X_test, y_val, y_test =  train_test_split(X_train, y_train,test_size=0.25)\r\n",
        "\r\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120, 4) (90, 4) (30, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsFUxNNL6kWJ",
        "outputId": "77ab57f8-6e02-485f-cd67-2026f7b66adc"
      },
      "source": [
        "# 15번 : Cross Validation\r\n",
        "# train / validation / test의 형태로 데이터를 나누어서 학습을 계속 진행하게 되면, validation data에 학습이 과적합되면서 성능이 제대로 향상되지 않는 문제가 있습니다.\r\n",
        "# 이 문제를 해결하기 위해서 학습시에 train/validation로 데이터를 여러 조합으로 나누는 방법이 제시되었습니다.\r\n",
        "# 이 방법을 Cross-Validation(CV)이라고 하며, 대표적으로 k-fold Cross Validation을 사용합니다.\r\n",
        "# 기존의 작성했던 코드를 활용하여 k-fold cross validation를 통해 학습하는 코드를 작성하세요. (단, k = 5)\r\n",
        "# code from 11번\r\n",
        "\r\n",
        "# KFold, Cross validation를 위한 라이브러리 로드.\r\n",
        "from sklearn.model_selection import KFold, cross_val_score\r\n",
        "# 11번 문제에서 만든 pipeline\r\n",
        "clf = pipe\r\n",
        "# 5-fold Cross validation을 통해 학습\r\n",
        "cv_results =cross_val_score(clf, iris.data, iris.target, cv=5)\r\n",
        "# 5-fold CV로 학습한 결과의 평균 정확도 출력\r\n",
        "cv_results.mean()\r\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9266666666666665"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzjyJe786wvU",
        "outputId": "4862ebc3-8122-4c88-aac4-d7e9a5cbf7e8"
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\r\n",
        "\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "\r\n",
        "pipeline = Pipeline([\r\n",
        "    ('minmaxscaler',MinMaxScaler()),\r\n",
        "    ('logisticregression', LogisticRegression())\r\n",
        "])\r\n",
        "\r\n",
        "clf = pipeline\r\n",
        "cv_results = cross_val_score(clf, X, y, cv=KFold(n_splits=5))\r\n",
        "cv_results.mean()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.72"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QWJAaJf9TSM",
        "outputId": "6f93d376-77fd-4519-858e-48552f398444"
      },
      "source": [
        "# 16번 : Hyper-parameter Tuning - 1\r\n",
        "# estimator의 성능 향상을 위해서 hyper-parameter를 조정해봅시다.\r\n",
        "# 문제의 예시로 계속 사용했던 LogisticRegression을 예시로 살펴봅시다.\r\n",
        "# 굉장히 많은 parameter가 LogisticRegression 모델의 학습에 영향을 끼치지만 그 중에서 C, solver가 학습에 가장 큰 영향을 끼칩니다.\r\n",
        "# C를 np.logspace(-4, 4, 4) 로 설정하고, solver는 가능한 모든 solver들(\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\") 로 설정합니다.\r\n",
        "# 설정한 hyperparameter 조합중 어떤 조합이 가장 좋은지 판단하는 코드를 작성하세요.\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import numpy as np\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "\r\n",
        "pipe = Pipeline([\r\n",
        "    ('minmaxscaler',MinMaxScaler()),\r\n",
        "    ('lr', LogisticRegression())\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import GridSearchCV # GridSearchCV 불러오기\r\n",
        "\r\n",
        "clf = pipe\r\n",
        "\r\n",
        "# GridSearch에 사용할 parameter들을 지정한 dictionary\r\n",
        "param_grid = {\r\n",
        "    'lr__C': np.logspace(-4, 4, 4),\r\n",
        "    'lr__solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\r\n",
        "    }\r\n",
        "\r\n",
        "# GridSearchCV를 불러와서 지정한 clf와 param_grid로 설정.\r\n",
        "search = GridSearchCV(clf, param_grid)\r\n",
        "search.fit(X_train, y_train)\r\n",
        "print(\"Best option from GridSearchCV : \", search.best_params_)\r\n",
        "\r\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best option from GridSearchCV :  {'lr__C': 10000.0, 'lr__solver': 'newton-cg'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM6p-v3Q996a",
        "outputId": "9179104c-cf71-483a-9928-6d2b33310381"
      },
      "source": [
        "# 17번 : Hyper-parameter Tuning - 2\r\n",
        "# 모든 조합을 탐색하는 GridSearch와 달리, 주어진 range에서 sampling을 통해 hyper-parameter를 정하는 RandomizedSearch 방법을 사용해봅시다.\r\n",
        "# C를 np.logspace(-4, 4, 4) 로 설정하고, solver는 가능한 모든 solver들(\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\") 로 설정합니다.\r\n",
        "# 설정한 hyperparameter 조합중 어떤 조합이 가장 좋은지 판단하는 코드를 작성하세요.\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "\r\n",
        "pipe = Pipeline([\r\n",
        "    ('minmaxscaler',MinMaxScaler()),\r\n",
        "    ('lr', LogisticRegression())\r\n",
        "])\r\n",
        "\r\n",
        "# Random SearchCV 불러오기\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "\r\n",
        "clf = pipe\r\n",
        "\r\n",
        "# RandomSearch에 사용할 parameter들을 지정한 dictionary\r\n",
        "param_distribution = {\r\n",
        "    'lr__C': np.logspace(-4, 4, 4),\r\n",
        "    'lr__solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\r\n",
        "}\r\n",
        "\r\n",
        "# Random Search를 불러와서 지정한 clf와 param_grid로 설정.\r\n",
        "search2 =RandomizedSearchCV(clf, param_distribution)\r\n",
        "# 찾은 best hyperparameter setting을 출력.\r\n",
        "search2.fit(X_train, y_train)\r\n",
        "print(\"Best option from RandomizedSearchCV : \", search2.best_params_)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best option from RandomizedSearchCV :  {'lr__solver': 'lbfgs', 'lr__C': 10000.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDlA5C3B_-vO",
        "outputId": "d485a466-2137-4e76-d7c8-db60fd265135"
      },
      "source": [
        "# 18번 : End-to-End\r\n",
        "# 이 때까지 진행했던 모든 코드를 모아서 하나의 완성된 코드를 만들어봅시다.\r\n",
        "# Iris dataset이 주어지면 cross-validation을 이용하여 best hyperparameter를 찾고 그 결과를 accuracy, f1-score로 출력하는 코드를 작성하고 최종 성능을 비교하세요.\r\n",
        "# 과연 제일 열심히 공부한 사람은 누구일까요?\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import numpy as np\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "\r\n",
        "pipe = Pipeline([\r\n",
        "    ('minmaxscaler',MinMaxScaler()),\r\n",
        "    ('lr', LogisticRegression())\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import GridSearchCV # GridSearchCV 불러오기\r\n",
        "\r\n",
        "clf = pipe\r\n",
        "\r\n",
        "# GridSearch에 사용할 parameter들을 지정한 dictionary\r\n",
        "param_grid = {\r\n",
        "    'lr__C': np.logspace(-4, 4, 4),\r\n",
        "    'lr__solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\r\n",
        "    }\r\n",
        "\r\n",
        "# GridSearchCV를 불러와서 지정한 clf와 param_grid로 설정.\r\n",
        "search = GridSearchCV(clf, param_grid)\r\n",
        "search.fit(X_train, y_train)\r\n",
        "print(\"Best option from GridSearchCV : \", search.best_params_)\r\n",
        "\r\n",
        "\r\n",
        "model = search.best_estimator_.fit(X_train, y_train)\r\n",
        "pred = model.predict(X_test)\r\n",
        "\r\n",
        "# Accuracy\r\n",
        "acc = accuracy_score(y_test, y_pred)\r\n",
        "# F1 score\r\n",
        "f1 = f1_score(y_test, y_pred,average=  'weighted')\r\n",
        "print(\"Accuracy : %.3f\" % acc)\r\n",
        "print(\"F1 score : %.3f\" % f1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best option from GridSearchCV :  {'lr__C': 10000.0, 'lr__solver': 'newton-cg'}\n",
            "Accuracy : 0.367\n",
            "F1 score : 0.363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtnNTiqhAkcC",
        "outputId": "91bb801a-92ae-4ca1-9b66-89ddb74d9926"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from sklearn.metrics import accuracy_score, f1_score\r\n",
        "\r\n",
        "\r\n",
        "# Iris dataset 불러오기\r\n",
        "X, y = load_iris(return_X_y=True)\r\n",
        "\r\n",
        "\r\n",
        "# train, test 8:2 split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\r\n",
        "\r\n",
        "\r\n",
        "# pipeline 정의\r\n",
        "pipeline = Pipeline([\r\n",
        "    ('minmaxscaler', MinMaxScaler()),\r\n",
        "    ('logisticregression', LogisticRegression())\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "clf = pipeline\r\n",
        "\r\n",
        "\r\n",
        "# clf.get_params().keys(): 모델 파라미터 확인\r\n",
        "# search할 파라미터 및 값 정의\r\n",
        "param_dist = {\r\n",
        "    'logisticregression__C': np.logspace(-4, 4, 4),\r\n",
        "    'logisticregression__solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "search = RandomizedSearchCV(clf, param_dist, cv = 5)\r\n",
        "search.fit(X_train, y_train)\r\n",
        "\r\n",
        "\r\n",
        "# random search를 통해 얻은 베스트 파라미터 pipeline\r\n",
        "clf = search.best_estimator_\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "\r\n",
        "y_pred = clf.predict(X_test)\r\n",
        "\r\n",
        "\r\n",
        "acc = accuracy_score(y_test, y_pred)\r\n",
        "f1 = f1_score(y_test, y_pred, average=None).mean()\r\n",
        "\r\n",
        "\r\n",
        "print(\"Accuracy : \", acc)\r\n",
        "print(\"F1-score : \", f1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  1.0\n",
            "F1-score :  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaw4s2jaAl86"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}