{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "스파크 완벽가이드.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10_Ol52tGLcDXvR11IiUxcAMR_xuTL11y",
      "authorship_tag": "ABX9TyOUXgb2DTsPWQSTsbRL+2oy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjs1995/study/blob/main/spark/%EC%8A%A4%ED%8C%8C%ED%81%AC_%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tq02yAj3H1H"
      },
      "source": [
        "# flightData2015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UN5lHcEgaQ9",
        "outputId": "bc03554f-1c8b-4e0f-b73b-dda0f91ff1c5"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7YH-Y_Rg9bY",
        "outputId": "254cf275-22a5-4aab-894e-8bc245c0cd38"
      },
      "source": [
        "!pip install spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7XIz9Kgd3b"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
        "\n",
        "flightData2015 = spark\\\n",
        "  .read\\\n",
        "  .option(\"inferSchema\", \"true\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .csv(\"/content/2015-summary.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K-xLUTsg8fm"
      },
      "source": [
        "# spark sql \n",
        "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHyZIkGUy8k1",
        "outputId": "5d00c89a-3abc-48ef-b362-6e2f5b938fdc"
      },
      "source": [
        "sqlWay = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, count(1)\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "\"\"\")\n",
        "\n",
        "dataFrameWay = flightData2015\\\n",
        "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "  .count()\n",
        "\n",
        "sqlWay.explain()\n",
        "dataFrameWay.explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), ENSURE_REQUIREMENTS, [id=#33]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), ENSURE_REQUIREMENTS, [id=#46]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs4gIABIzHM_",
        "outputId": "b47ba834-8fe1-4992-efae-ffd29cd3fbcf"
      },
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "flightData2015.select(max(\"count\")).take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW2gGYvGzK5G",
        "outputId": "eb5004d5-f912-4b6f-c99b-ece8da714b25"
      },
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "ORDER BY sum(count) DESC\n",
        "LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "maxSql.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdmMp1JszLcp",
        "outputId": "5b657aff-5684-4e8f-b438-8d7bb94b5f36"
      },
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "flightData2015\\\n",
        "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "  .sum(\"count\")\\\n",
        "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "  .sort(desc(\"destination_total\"))\\\n",
        "  .limit(5)\\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3nj5SzzzN--",
        "outputId": "eb1426a0-101b-468b-872d-76758669ae93"
      },
      "source": [
        "flightData2015\\\n",
        "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "  .sum(\"count\")\\\n",
        "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "  .sort(desc(\"destination_total\"))\\\n",
        "  .limit(5)\\\n",
        "  .explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#97L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#16,destination_total#97L])\n",
            "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(count#18)])\n",
            "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), ENSURE_REQUIREMENTS, [id=#214]\n",
            "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(count#18)])\n",
            "            +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBSDuSDU3HAW"
      },
      "source": [
        "# retail-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUj7ygAwzPzF"
      },
      "source": [
        "  spark=SparkSession.builder.appName('retail').getOrCreate()\n",
        "\n",
        "  staticDataFrame = spark.read.format(\"csv\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .option(\"inferSchema\", \"true\")\\\n",
        "  .load(\"/content/drive/MyDrive/streamlit/by-day/*.csv\")\n",
        "\n",
        "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
        "staticSchema = staticDataFrame.schema\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApMyO2rb3gg4",
        "outputId": "c4084c14-8259-4de7-e9c9-df44afd9ee83"
      },
      "source": [
        "from pyspark.sql.functions import window, column, desc, col\n",
        "staticDataFrame\\\n",
        "  .selectExpr(\n",
        "    \"CustomerId\",\n",
        "    \"(UnitPrice * Quantity) as total_cost\",\n",
        "    \"InvoiceDate\")\\\n",
        "  .groupBy(\n",
        "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
        "  .sum(\"total_cost\")\\\n",
        "  .show(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----------------+\n",
            "|CustomerId|              window|  sum(total_cost)|\n",
            "+----------+--------------------+-----------------+\n",
            "|   16057.0|{2011-12-05 00:00...|            -37.6|\n",
            "|   14126.0|{2011-11-29 00:00...|643.6300000000001|\n",
            "|   13500.0|{2011-11-16 00:00...|497.9700000000001|\n",
            "|   17160.0|{2011-11-08 00:00...|516.8499999999999|\n",
            "|   15608.0|{2011-11-11 00:00...|            122.4|\n",
            "+----------+--------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4refbqrr3hw4"
      },
      "source": [
        "streamingDataFrame = spark.readStream\\\n",
        "    .schema(staticSchema)\\\n",
        "    .option(\"maxFilesPerTrigger\", 1)\\\n",
        "    .format(\"csv\")\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .load(\"/content/drive/MyDrive/streamlit/by-day/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTvxUNIA3jvB"
      },
      "source": [
        "purchaseByCustomerPerHour = streamingDataFrame\\\n",
        "  .selectExpr(\n",
        "    \"CustomerId\",\n",
        "    \"(UnitPrice * Quantity) as total_cost\",\n",
        "    \"InvoiceDate\")\\\n",
        "  .groupBy(\n",
        "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
        "  .sum(\"total_cost\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "Mt2DHOlW3lOm",
        "outputId": "61bebc8f-ff20-4cd3-bfb1-ed1614ba9fcd"
      },
      "source": [
        "purchaseByCustomerPerHour.writeStream\\\n",
        "    .format(\"memory\")\\\n",
        "    .queryName(\"customer_purchases\")\\\n",
        "    .outputMode(\"complete\")\\\n",
        "    .start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-0e228513cb27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpurchaseByCustomerPerHour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customer_purchases\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name customer_purchases as a query with that name is already active in this SparkSession"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHjulG3d3m6N",
        "outputId": "3fb29f38-6787-4ee9-d9ef-a1c7d42f7e74"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "  SELECT *\n",
        "  FROM customer_purchases\n",
        "  ORDER BY `sum(total_cost)` DESC\n",
        "  \"\"\")\\\n",
        "  .show(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---------------+\n",
            "|CustomerId|window|sum(total_cost)|\n",
            "+----------+------+---------------+\n",
            "+----------+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnXpeckG3oLN"
      },
      "source": [
        "from pyspark.sql.functions import date_format, col\n",
        "preppedDataFrame = staticDataFrame\\\n",
        "  .na.fill(0)\\\n",
        "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
        "  .coalesce(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Q-BC0T3oF8"
      },
      "source": [
        "trainDataFrame = preppedDataFrame\\\n",
        "  .where(\"InvoiceDate < '2011-07-01'\")\n",
        "testDataFrame = preppedDataFrame\\\n",
        "  .where(\"InvoiceDate >= '2011-07-01'\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP5k3Kww3qAk"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer()\\\n",
        "  .setInputCol(\"day_of_week\")\\\n",
        "  .setOutputCol(\"day_of_week_index\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXDISYT43r2-"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "encoder = OneHotEncoder()\\\n",
        "  .setInputCol(\"day_of_week_index\")\\\n",
        "  .setOutputCol(\"day_of_week_encoded\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2055sA_3sn1"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "vectorAssembler = VectorAssembler()\\\n",
        "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
        "  .setOutputCol(\"features\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "QKGHd91I3tk8",
        "outputId": "a28b3f7f-ff9f-400b-c457-6759e56eebe9"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "transformationPipeline = Pipeline()\\\n",
        "  .setStages([indexer, encoder, vectorAssembler])\n",
        "\n",
        "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
        "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
        "\n",
        "from pyspark.ml.clustering import KMeans\n",
        "kmeans = KMeans()\\\n",
        "  .setK(20)\\\n",
        "  .setSeed(1L)\n",
        "\n",
        "kmModel = kmeans.fit(transformedTraining)\n",
        "transformedTest = fittedPipeline.transform(testDataFrame)\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-49ac56095394>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    kmeans = KMeans()  .setK(20)  .setSeed(1L)\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}