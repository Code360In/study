{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_NLP_PySpark.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7dSiomkWY3A"
      },
      "source": [
        "# NLP_PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vh9MQpxU8jY"
      },
      "source": [
        "SparkML도 다른 머신러닝 라이브러리들과 마찬가지로 학습을 위한 전처리, 모델 알고리즘, 성능을 극대화하기 위한 도구들을 지원합니다. 다만, 다른 라이브러리에 비해 스파크는 대중적으로 사용되는 몇몇 알고리즘만 구현되어 있습니다. 새롭거나 핫한 모델이 나와도 스파크에서 쓰려면 다른 라이브러리보다는 조금 더 기다려야 합니다.\r\n",
        "\r\n",
        "학습에 필요한 전처리를 스파크로 진행하고 모델링은 텐서플로우와 같은 타 라이브러리로 진행하거나, 스파크 지원 모델로 충분한 프로젝트라면 모델링까지 스파크로 마무리하여 작업의 속도를 높일 수 있습니다.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzlUDTJvSdYt"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-IREO_SCNw"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "spark=SparkSession.builder.appName('nlp').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTkwyfQFSbYj"
      },
      "source": [
        "df=spark.createDataFrame([(1,'I really liked this movie'),\r\n",
        "                         (2,'I would recommend this movie to my friends'),\r\n",
        "                         (3,'movie was alright but acting was horrible'),\r\n",
        "                         (4,'I am never watching that movie ever again')],\r\n",
        "                        ['user_id','review'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDY3o_TmScsX"
      },
      "source": [
        "df.show(5,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIAoeZ_QSwje"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Bpc8tNSyYI"
      },
      "source": [
        "tokenization=Tokenizer(inputCol='review',outputCol='tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6koxQ7TISzJN"
      },
      "source": [
        "tokenized_df=tokenization.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-D9kmYvS0bm"
      },
      "source": [
        "tokenized_df.show(4,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H8Wwd3ITNtf"
      },
      "source": [
        "StringIndexer -  StringIndexer로 변환해서 0, 1, 2로 바꿔줍니다. 라벨뿐만 아니라 문자열 피쳐에도 숫자 인덱스로 변환할 때\r\n",
        "\r\n",
        "tokenizer - MK는 밥을 먹는다라는 문장을 [MK는, 밥을, 먹는다]라는 토큰으로 나누어 줍니다.\r\n",
        "\r\n",
        "remover - 은, 는, 이, 가, 을, 를 처럼 조사 제거 [MK, 밥, 먹는다] \r\n",
        "\r\n",
        "word2vec - 단어 목록의 패턴을 계산해서 단어와 단어 사이의 관계를 수치로 표현하는 방법 [MK, 밥, 먹는다], [JK, 밥, 먹는다], [고양이, 소파, 눕는다]\r\n",
        "\r\n",
        "TF-IDF - 문장(review)에 들어있는 모든 단어를 세면 그게 Term Frequency(TF)\r\n",
        "[이거 게임 정말 좋아 정말 최고야], [이거 게임 별로임]\r\n",
        "* 일련의 문장으로 시작합니다. 를 사용하여 각 문장을 단어로 나눕니다 Tokenizer. 각 문장 (단어 HashingTF모음)에 대해 문장을 특징 벡터로 해시하는 데 사용 합니다. IDF특징 벡터의 크기를 조정하는 데 사용 합니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2V_COyjS1Eg"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC8EbKN2S2z-"
      },
      "source": [
        "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDrO1BVrS339"
      },
      "source": [
        "refined_df=stopword_removal.transform(tokenized_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSc8Ij9gTmi0"
      },
      "source": [
        "refined_df.select(['user_id','tokens','refined_tokens']).show(10,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXSPQOm6ToCp"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oss9-V1nTtxs"
      },
      "source": [
        "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChmTi4KsTu03"
      },
      "source": [
        "cv_df=count_vec.fit(refined_df).transform(refined_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rImO6vo1Tvh1"
      },
      "source": [
        "cv_df.select(['user_id','refined_tokens','features']).show(4,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHC18EKNTwlM"
      },
      "source": [
        "count_vec.fit(refined_df).vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fokCmwECTx3u"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF,IDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaIt4_98TzgG"
      },
      "source": [
        "hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2OOyULT0ch"
      },
      "source": [
        "hashing_df=hashing_vec.transform(refined_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BIRYQAKUI_F"
      },
      "source": [
        "hashing_df.select(['user_id','refined_tokens','tf_features']).show(4,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0YBi_4NUKCP"
      },
      "source": [
        "tf_idf_vec=IDF(inputCol='tf_features',outputCol='tf_idf_features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C4jG32lUM5L"
      },
      "source": [
        "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7aGt8LfUjzi"
      },
      "source": [
        "tf_idf_df.select(['user_id','tf_idf_features']).show(4,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_1uGM9xUkmY"
      },
      "source": [
        "text_df=spark.read.csv('/content/drive/MyDrive/datacamp/Movie_reviews.csv',inferSchema=True,header=True,sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oWWN-HaUpev"
      },
      "source": [
        "text_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkCdrhtkUqUZ"
      },
      "source": [
        "text_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpQnd_3FUrWJ"
      },
      "source": [
        "from pyspark.sql.functions import rand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdMYUDbPUsLc"
      },
      "source": [
        "text_df.orderBy(rand()).show(10,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKxub1fnUs3T"
      },
      "source": [
        "text_df=text_df.filter(((text_df.Sentiment =='1') | (text_df.Sentiment =='0')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGli3S0LUucl"
      },
      "source": [
        "text_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "derVy66cUvSi"
      },
      "source": [
        "text_df.groupBy('Sentiment').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8XsEpeCUwf3"
      },
      "source": [
        "text_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzdQ4wBCUxXS"
      },
      "source": [
        "text_df = text_df.withColumn(\"Label\", text_df.Sentiment.cast('float')).drop('Sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL9TyKdPUzvk"
      },
      "source": [
        "text_df.orderBy(rand()).show(10,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lHD2FVVU1Uv"
      },
      "source": [
        "text_df.groupBy('label').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb5qu0WxU2iX"
      },
      "source": [
        "from pyspark.sql.functions import length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF9o0xHBVF8Z"
      },
      "source": [
        "text_df=text_df.withColumn('length',length(text_df['Review']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8RVii_fVGo3"
      },
      "source": [
        "text_df.orderBy(rand()).show(10,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-pChN95VHRy"
      },
      "source": [
        "text_df.groupBy('Label').agg({'Length':'mean'}).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEfXNoEFVIVS"
      },
      "source": [
        "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWCqA4foVJlZ"
      },
      "source": [
        "tokenized_df=tokenization.transform(text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bVKsqvLVKd1"
      },
      "source": [
        "tokenized_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFPAbo7hVLI9"
      },
      "source": [
        "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGuBYlA2VMRP"
      },
      "source": [
        "refined_text_df=stopword_removal.transform(tokenized_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJpA11vfVNMW"
      },
      "source": [
        "refined_text_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT28JUFRVN4k"
      },
      "source": [
        "from pyspark.sql.functions import udf\r\n",
        "from pyspark.sql.types import IntegerType\r\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lre2tk5aVPF7"
      },
      "source": [
        "len_udf = udf(lambda s: len(s), IntegerType())\r\n",
        "\r\n",
        "refined_text_df = refined_text_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3U4zGz7VQgF"
      },
      "source": [
        "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Ta1Tk5VVFW"
      },
      "source": [
        "cv_text_df=count_vec.fit(refined_text_df).transform(refined_text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdvVXQF4VWNl"
      },
      "source": [
        "cv_text_df.select(['refined_tokens','token_count','features','Label']).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRha0EKcVXR9"
      },
      "source": [
        "model_text_df=cv_text_df.select(['features','token_count','Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V6KtdhAVYj1"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dPLumdFVZjD"
      },
      "source": [
        "df_assembler = VectorAssembler(inputCols=['features','token_count'],outputCol='features_vec')\r\n",
        "model_text_df = df_assembler.transform(model_text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z33zMnjfVfvf"
      },
      "source": [
        "model_text_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlkeX386Vg3y"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihhv-PUzVh7q"
      },
      "source": [
        "training_df,test_df=model_text_df.randomSplit([0.75,0.25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ifATLyTViui"
      },
      "source": [
        "training_df.groupBy('Label').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT9i-AvwVjiK"
      },
      "source": [
        "test_df.groupBy('Label').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SoazCH4VkO0"
      },
      "source": [
        "log_reg=LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikt0jm-yVlWt"
      },
      "source": [
        "results=log_reg.evaluate(test_df).predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT8AAQgxVmB1"
      },
      "source": [
        "results.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQQHnIWOVm8S"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP8hkvJhVoCR"
      },
      "source": [
        "true_postives = results[(results.Label == 1) & (results.prediction == 1)].count()\r\n",
        "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()\r\n",
        "false_positives = results[(results.Label == 0) & (results.prediction == 1)].count()\r\n",
        "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036zvZ3_VpPa"
      },
      "source": [
        "recall = float(true_postives)/(true_postives + false_negatives)\r\n",
        "print(recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0pxCRQVqAX"
      },
      "source": [
        "precision = float(true_postives) / (true_postives + false_positives)\r\n",
        "print(precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VjHS1FKVqxD"
      },
      "source": [
        "accuracy=float((true_postives+true_negatives) /(results.count()))\r\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjRvv72jVrdh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ov-MmjtWcSt"
      },
      "source": [
        "# Sequence_Embeddings_PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KhTUL7yWdox"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "from pyspark.ml.feature import StringIndexer\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.dummy import DummyClassifier\r\n",
        "from statsmodels.api import Logit\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "import itertools\r\n",
        "import re\r\n",
        "from random import sample\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVpQGMXVWk6a"
      },
      "source": [
        "from gensim.models.doc2vec import LabeledSentence\r\n",
        "from gensim.models import Doc2Vec\r\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5YfHLKeWnjW"
      },
      "source": [
        "spark=SparkSession.builder.appName('seq_embedding').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt1ryNOLWr8h"
      },
      "source": [
        "df = spark.read.csv('embedding_dataset.csv',header=True,inferSchema=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}